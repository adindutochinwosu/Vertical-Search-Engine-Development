{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Q-DqQZuVaa0N"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kosit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kosit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import requests\n",
    "import operator\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from time import sleep\n",
    "from random import randint\n",
    "\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8KXLZbJAqBK"
   },
   "source": [
    "# Get the HTML tags and content of a web page\n",
    "Letâ€™s define a function to request and parse a HTML web page as we will need this a lot during this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleScholarScraper:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        \n",
    "        \n",
    "    # request and parse a HTML web page\n",
    "    def getAndParseURL(self, url):\n",
    "        \"\"\"\n",
    "        Parses the HTML web page and returns the content\n",
    "        and HTML tags for the page\n",
    "\n",
    "        args:\n",
    "          url: URL to a web page\n",
    "\n",
    "        \"\"\"\n",
    "        result = requests.get(url)\n",
    "        soup = BeautifulSoup(result.text, 'html.parser') \n",
    "        \n",
    "        return soup\n",
    "    \n",
    "    \n",
    "    # retrieve the Staff profile URLs on any page\n",
    "    def getStaffProfileURLs(self, url):\n",
    "        \"\"\"\n",
    "        Gets all the Staff Profile URLs for any given web page\n",
    "\n",
    "        args:\n",
    "          url: URL to a web page\n",
    "\n",
    "        returns:\n",
    "          A list of all Staff Profile URLs on a given web page\n",
    "        \"\"\"\n",
    "        # get the tags and webpage content\n",
    "\n",
    "        # remove the citation part of the base url and\n",
    "        # replace with the citation part for each Staff\n",
    "        \n",
    "        soup = self.getAndParseURL(url)\n",
    "        \n",
    "        listOfStaffURLs = [\"/\".join(url.split(\"/\")[:-1]) + x.div.a.get(\"href\") \\\n",
    "                         for x in soup.findAll(\"div\", class_=\"gsc_1usr\")]\n",
    "        \n",
    "        return listOfStaffURLs\n",
    "    \n",
    "    \n",
    "    def getAllProfilePagesURL(self, url):\n",
    "        \"\"\"\n",
    "        Gets the URL of all profile pages of Coventry Univesity on Google Scholar \n",
    "\n",
    "        args:\n",
    "          url: the starting or main page\n",
    "\n",
    "        returns:\n",
    "          pages_urls: a list of URLs for all Coventry University Google Scholar \n",
    "                      Profile pages\n",
    "        \"\"\"\n",
    "\n",
    "        # store the URLs of all the pages\n",
    "        pages_urls = [url]\n",
    "\n",
    "        # get the html tags and contents of the main page\n",
    "        soup = self.getAndParseURL(pages_urls[0])\n",
    "\n",
    "        cite = soup.find(\"button\", class_=\"gs_btnPR gs_in_ib gs_btn_half gs_btn_lsb gs_btn_srt gsc_pgn_pnx\") \\\n",
    "        .get(\"onclick\").split(\"/\")[1].replace(\"\\\\x3d\",\"=\").replace(\"\\\\x26\", \"&\").replace(\"&oe=ASCII&\", \"&\") \\\n",
    "        .split(\"'\")[0]\n",
    "        \n",
    "        new_url = \"https://\" + self.url.split(\"//\")[1].split(\"/\")[0] + \"/\" + cite\n",
    "\n",
    "        while requests.get(new_url).status_code == 200:\n",
    "            pages_urls.append(new_url)\n",
    "\n",
    "            try:\n",
    "                # Controlling the crawl rate\n",
    "                sleep(randint(2,10))\n",
    "\n",
    "                soup = self.getAndParseURL(new_url)\n",
    "                cite = soup.find(\"button\", class_=\"gs_btnPR gs_in_ib gs_btn_half gs_btn_lsb gs_btn_srt gsc_pgn_pnx\") \\\n",
    "                .get(\"onclick\").split(\"/\")[1].replace(\"\\\\x3d\",\"=\").replace(\"\\\\x26\", \"&\") \\\n",
    "                .replace(\"&oe=ASCII&\", \"&\").split(\"'\")[0]\n",
    "                \n",
    "                new_url = \"https://\" + self.url.split(\"//\")[1].split(\"/\")[0] + \"/\" + cite\n",
    "\n",
    "            except AttributeError:\n",
    "                break\n",
    "\n",
    "        return pages_urls\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getAllStaffProfileURLs(self, url):\n",
    "        \"\"\"\n",
    "        Gets the profile url of every member of Staff on each Coventry University\n",
    "        Google Scholar Profile page.\n",
    "\n",
    "        args:\n",
    "          pages_urls: a list of URLs for all Coventry University Google Scholar \n",
    "                      Profile pages\n",
    "\n",
    "        returns:\n",
    "          staffURLs: a list of URLs of all the profiles of Coventry University \n",
    "                     member of Staff on Google Scholar\n",
    "        \"\"\"\n",
    "        pages_urls = self.getAllProfilePagesURL(url)\n",
    "        allStaffURLs = []\n",
    "\n",
    "        for page in pages_urls:\n",
    "            # get all Staff profile URLs on each page\n",
    "            allStaffURLs.extend(self.getStaffProfileURLs(page))\n",
    "\n",
    "            # Controlling the crawl rate\n",
    "            sleep(randint(2,10))\n",
    "\n",
    "        return allStaffURLs\n",
    "    \n",
    "    \n",
    "    \n",
    "    def getAllPublications(self, url):\n",
    "    \n",
    "        publications = {\"staffName\" : [],\n",
    "                    \"researchArea\" : [],\n",
    "                    \"title\" : [],\n",
    "                    \"cited\" : [],\n",
    "                    \"year\" : []}\n",
    "        \n",
    "        # Get the URLs of each Staff on all pages\n",
    "        allStaffProfileURLs = self.getAllStaffProfileURLs(url)\n",
    "        \n",
    "        \n",
    "        # get all the publications for on each Staff profile page\n",
    "        for staffProfileURL in allStaffProfileURLs:\n",
    "\n",
    "            staffName = ''\n",
    "            researchAreas = []\n",
    "            publication_titles = []\n",
    "            num_citations = []\n",
    "            year_publication = []\n",
    "\n",
    "\n",
    "            # get the tags and webpage content on each Staff profile page\n",
    "            soup = self.getAndParseURL(staffProfileURL)\n",
    "\n",
    "\n",
    "            # get the staff name\n",
    "            staffName = soup.find(\"div\", {\"id\":\"gsc_prf_in\"}).text\n",
    "\n",
    "\n",
    "            # get the list of research area\n",
    "            for div in soup.findAll(\"div\", {\"class\" : \"gsc_prf_il\"}):\n",
    "                area_tags = div.findAll(\"a\")\n",
    "\n",
    "            for area in area_tags:\n",
    "                researchAreas.append(area.get_text())\n",
    "\n",
    "\n",
    "            # get the title of each publication\n",
    "            publication_tags = soup.find_all(\"td\", class_=\"gsc_a_t\")\n",
    "            for publication in publication_tags:\n",
    "                publication_titles.append(publication.find(\"a\").text)\n",
    "\n",
    "\n",
    "            # Controlling the crawl rate\n",
    "            sleep(randint(2,10))\n",
    "\n",
    "            # get the number of citations for each publication\n",
    "            cited_tags = soup.find_all(\"a\", class_=\"gsc_a_ac gs_ibl\")\n",
    "            for cited in cited_tags:\n",
    "                num_citations.append(cited.text)\n",
    "\n",
    "\n",
    "            # get the year of publication for each publication\n",
    "            year_tags = soup.find_all(\"span\", class_=\"gsc_a_h gsc_a_hc gs_ibl\")\n",
    "            for year in year_tags:\n",
    "                year_publication.append(year.text)\n",
    "\n",
    "\n",
    "            publications[\"staffName\"].append(staffName)\n",
    "            publications[\"researchArea\"].append(researchAreas)\n",
    "            publications[\"title\"].append(publication_titles)\n",
    "            publications[\"cited\"].append(num_citations)\n",
    "            publications[\"year\"].append(year_publication)\n",
    "            \n",
    "            \n",
    "        return publications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to save and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveAndLoadData(data, file_path = \"file_path\"):\n",
    "    \n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    \n",
    "    with open (file_path, 'rb') as fp:\n",
    "        load_data = pickle.load(fp)\n",
    "        \n",
    "    return load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./data/scraped_publications\"\n",
    "with open (file_path, 'rb') as fp:\n",
    "    publications_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Solution\n",
    "\n",
    "#### TODO: check if data exist in file before crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>staffName</th>\n",
       "      <th>researchArea</th>\n",
       "      <th>title</th>\n",
       "      <th>cited</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Timothy Mason</td>\n",
       "      <td>[sonochemistry, ultrasound, chemistry, environ...</td>\n",
       "      <td>[Sonochemistry, Applied sonochemistry: the use...</td>\n",
       "      <td>[2068, 1503, 1178, 910, 798, 667, 475, 459, 42...</td>\n",
       "      <td>[1988, 2002, 1996, 1991, 1997, 2001, 2010, 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gurnam Singh</td>\n",
       "      <td>[social work, race and racism, critical pedago...</td>\n",
       "      <td>[Mitochondrial DNA mutation associated with Le...</td>\n",
       "      <td>[2629, 2554, 400, 364, 299, 249, 245, 214, 205...</td>\n",
       "      <td>[1988, 2005, 2000, 1981, 2006, 2007, 1960, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WD Li</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Evaluation of the role of phonological STM in...</td>\n",
       "      <td>[1788, 1117, 588, 384, 314, 306, 230, 226, 213...</td>\n",
       "      <td>[1989, 1992, 2013, 2013, 2015, 2005, 2007, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dr. Mohammad M Ali</td>\n",
       "      <td>[Forecast Information Sharing, ARIMA Modelling...</td>\n",
       "      <td>[Aquatic toxicity from pulp and paper mill eff...</td>\n",
       "      <td>[821, 604, 579, 182, 105, 104, 99, 95, 93, 91,...</td>\n",
       "      <td>[2001, 1995, 2004, 2013, 2013, 1995, 2000, 201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petra Wark</td>\n",
       "      <td>[m/eHealth, epidemiology, primary prevention, ...</td>\n",
       "      <td>[World malaria report 2015, Alcohol attributab...</td>\n",
       "      <td>[7563, 343, 279, 271, 235, 234, 153, 141, 132,...</td>\n",
       "      <td>[2016, 2011, 2012, 2013, 2014, 2013, 2013, 201...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            staffName                                       researchArea  \\\n",
       "0       Timothy Mason  [sonochemistry, ultrasound, chemistry, environ...   \n",
       "1        Gurnam Singh  [social work, race and racism, critical pedago...   \n",
       "2               WD Li                                                 []   \n",
       "3  Dr. Mohammad M Ali  [Forecast Information Sharing, ARIMA Modelling...   \n",
       "4          Petra Wark  [m/eHealth, epidemiology, primary prevention, ...   \n",
       "\n",
       "                                               title  \\\n",
       "0  [Sonochemistry, Applied sonochemistry: the use...   \n",
       "1  [Mitochondrial DNA mutation associated with Le...   \n",
       "2  [Evaluation of the role of phonological STM in...   \n",
       "3  [Aquatic toxicity from pulp and paper mill eff...   \n",
       "4  [World malaria report 2015, Alcohol attributab...   \n",
       "\n",
       "                                               cited  \\\n",
       "0  [2068, 1503, 1178, 910, 798, 667, 475, 459, 42...   \n",
       "1  [2629, 2554, 400, 364, 299, 249, 245, 214, 205...   \n",
       "2  [1788, 1117, 588, 384, 314, 306, 230, 226, 213...   \n",
       "3  [821, 604, 579, 182, 105, 104, 99, 95, 93, 91,...   \n",
       "4  [7563, 343, 279, 271, 235, 234, 153, 141, 132,...   \n",
       "\n",
       "                                                year  \n",
       "0  [1988, 2002, 1996, 1991, 1997, 2001, 2010, 199...  \n",
       "1  [1988, 2005, 2000, 1981, 2006, 2007, 1960, 200...  \n",
       "2  [1989, 1992, 2013, 2013, 2015, 2005, 2007, 200...  \n",
       "3  [2001, 1995, 2004, 2013, 2013, 1995, 2000, 201...  \n",
       "4  [2016, 2011, 2012, 2013, 2014, 2013, 2013, 201...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Google Scholar page for Coventry University:\n",
    "main_url = \"https://scholar.google.co.uk/citations?view_op=view_org&hl=en&org=9117984065169182779\"\n",
    "\n",
    "# Instantiate the crawler\n",
    "crawler = GoogleScholarScraper(main_url)\n",
    "\n",
    "# Get the URLs of each Staff on all pages\n",
    "# publications_dict = crawl.getAllPublications(main_url)      # commented to to be able to run the code\n",
    "\n",
    "# save scraped data\n",
    "file_path = \"./data/scraped_publications\"\n",
    "unprocessed_publications = saveAndLoadData(publications_dict, file_path)\n",
    "\n",
    "# stored unprocessed data in a dataframe\n",
    "unprocessed_publications_df = pd.DataFrame.from_dict(unprocessed_publications)\n",
    "unprocessed_publications_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "### Select name of Staff and publication titles, and unpack into a dataframe\n",
    "\n",
    "Observing the dataset shows that there are publications with missing publications and number of times the publications have been cited. So we will only work with the names and publication titles of the Staff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>StaffName</th>\n",
       "      <th>Titles</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Timothy Mason</td>\n",
       "      <td>Sonochemistry</td>\n",
       "      <td>1988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Timothy Mason</td>\n",
       "      <td>Applied sonochemistry: the uses of power ultra...</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Timothy Mason</td>\n",
       "      <td>The uses of ultrasound in food technology</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Timothy Mason</td>\n",
       "      <td>Practical sonochemistry</td>\n",
       "      <td>1991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Timothy Mason</td>\n",
       "      <td>Ultrasound in synthetic organic chemistry</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       StaffName                                             Titles  Year\n",
       "0  Timothy Mason                                      Sonochemistry  1988\n",
       "1  Timothy Mason  Applied sonochemistry: the uses of power ultra...  2002\n",
       "2  Timothy Mason          The uses of ultrasound in food technology  1996\n",
       "3  Timothy Mason                            Practical sonochemistry  1991\n",
       "4  Timothy Mason          Ultrasound in synthetic organic chemistry  1997"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the staff names and publications title\n",
    "pub_Staff_Title = unprocessed_publications_df[[\"staffName\", \"title\", \"year\"]]\n",
    "\n",
    "# create list to hold name of each staff and the publications\n",
    "name = []\n",
    "title = []\n",
    "publication_year = []\n",
    "\n",
    "# loop through each row, then loop through list of publications\n",
    "# titles, and extract each title, and also the name of the Staff\n",
    "for i in range(len(pub_Staff_Title[\"title\"])):\n",
    "    for x in range(len(pub_Staff_Title[\"title\"][i])):\n",
    "        title.append(pub_Staff_Title[\"title\"][i][x])\n",
    "        name.append(pub_Staff_Title[\"staffName\"][i])\n",
    "        \n",
    "        \n",
    "# Extract the publication year for each publication\n",
    "for i in range(len(pub_Staff_Title[\"title\"])):\n",
    "    for x in range(len(pub_Staff_Title[\"year\"][i])):\n",
    "        year = pub_Staff_Title[\"year\"][i][x]\n",
    "        \n",
    "        if year is None:\n",
    "            publication_year.append(\"missing\")\n",
    "        else:\n",
    "            publication_year.append(year)\n",
    "        \n",
    "        \n",
    "# create dictionary of Staff name and publications\n",
    "processed_publications_dict = {\"StaffName\" : name,\n",
    "                               \"Titles\" : title,\n",
    "                              \"Year\" : publication_year}\n",
    "\n",
    "# create dataframe of Staff name and publications\n",
    "processed_publications = pd.DataFrame(processed_publications_dict)\n",
    "\n",
    "# save processed data\n",
    "file_path = \"./data/processed_publications\"\n",
    "processed_publications = saveAndLoadData(processed_publications, file_path)\n",
    "\n",
    "# display first 5 rows\n",
    "processed_publications.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10212, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_publications.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the final data to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the titles to a list\n",
    "publication_titles = processed_publications['Titles'].values.tolist()\n",
    "\n",
    "# saved the list of publication titles to file\n",
    "file_path = \"./data/publication_titles\"\n",
    "publication_titles = saveAndLoadData(publication_titles, file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEngine:\n",
    "    \n",
    "    def __init__(self, publications, stopwords, query):\n",
    "        self.publications = publications\n",
    "        self.stopwords = stopwords\n",
    "        self.query = query\n",
    "        \n",
    "        \n",
    "    def tokenize(self, publications):\n",
    "        \"\"\"\n",
    "            Create tokens or distinct words from a string.\n",
    "\n",
    "            args:\n",
    "                document: document or search query to tokenize\n",
    "\n",
    "            returns:\n",
    "                tokens_list: list of lists of token\n",
    "        \"\"\"\n",
    "        tokens = re.findall('\\w+', str(publications).lower())\n",
    "        tokens_list = [word for word in tokens if word not in self.stopwords and word.isalpha()]\n",
    "\n",
    "        return tokens_list\n",
    "    \n",
    "    \n",
    "    def create_tokens(self):\n",
    "        \n",
    "        tokens = [self.tokenize(p) for p in self.publications]\n",
    "        \n",
    "        return tokens\n",
    "        \n",
    "    \n",
    "    def create_inverted_index(self):\n",
    "        \"\"\"\n",
    "        Create an inverted index given a list of document tokens. The index maps\n",
    "        each unique word to a list of document ids, sorted in increasing order.\n",
    "\n",
    "        args:\n",
    "          tokens...A list of lists of strings\n",
    "        returns:\n",
    "          An inverted index. This is a dict where keys are words and values are\n",
    "          lists of document indices, sorted in increasing order.\"\"\"\n",
    "\n",
    "        tokens = self.create_tokens()\n",
    "        inverted_index = {}\n",
    "        \n",
    "        for idx, document in enumerate(tokens):\n",
    "            for token in document:\n",
    "                if token not in inverted_index:\n",
    "                    inverted_index[token] = [idx]\n",
    "                else:\n",
    "                    inverted_index[token].append(idx)\n",
    "                    \n",
    "        return inverted_index\n",
    "    \n",
    "    \n",
    "    def sort_by_num_postings(self):\n",
    "\n",
    "        num_dict = {}\n",
    "        \n",
    "        token_query = self.tokenize(self.query)\n",
    "        inverted_index = self.create_inverted_index()\n",
    "        \n",
    "        for word in token_query:\n",
    "            num_dict[word] = len(inverted_index[word])\n",
    "        num_dict = sorted(num_dict, key=num_dict.get)\n",
    "\n",
    "        return num_dict\n",
    "    \n",
    "    \n",
    "    def publication_search(self):\n",
    "    \n",
    "        token_indexes = []\n",
    "\n",
    "        # get dictionary of sorted tokens\n",
    "        tokens = self.sort_by_num_postings()\n",
    "        \n",
    "        # return the indexes of the publications\n",
    "        indexes = self.create_inverted_index()\n",
    "\n",
    "        for token in tokens:\n",
    "            token_indexes.extend(indexes[token])\n",
    "\n",
    "        cnt = dict(Counter(token_indexes))\n",
    "        sorted_cnt = sorted(cnt.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        sorted_indexes = {k: v for k, v in sorted_cnt}\n",
    "\n",
    "        return sorted_indexes.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Search\n",
    "### if search word is not a word, return nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTER QUERY: erosion\n",
      "\n",
      "{'StaffName': 'Professor Damian Lawler', 'Titles': 'The measurement of river bank erosion and lateral channel change: a review', 'Year': '1993'}\n",
      "\n",
      "{'StaffName': 'Professor Damian Lawler', 'Titles': 'Process dominance in bank erosion systems', 'Year': '1992'}\n",
      "\n",
      "{'StaffName': 'Professor Damian Lawler', 'Titles': 'Downstream change in river bank erosion rates in the Swaleâ€“Ouse system, northern England', 'Year': '1999'}\n",
      "\n",
      "{'StaffName': 'Professor Damian Lawler', 'Titles': 'Bank erosion and instability', 'Year': '1997'}\n",
      "\n",
      "{'StaffName': 'Professor Damian Lawler', 'Titles': 'River bank erosion and the influence of frost: a statistical examination', 'Year': '1986'}\n",
      "\n",
      "{'StaffName': 'Professor Damian Lawler', 'Titles': 'A new technique for the automatic monitoring of erosion and deposition rates', 'Year': '1991'}\n",
      "\n",
      "{'StaffName': 'Professor Damian Lawler', 'Titles': 'Bank erosion events and processes in the Upper Severn basin', 'Year': '1997'}\n",
      "\n",
      "{'StaffName': 'Professor Damian Lawler', 'Titles': 'The importance of high-resolution monitoring in erosion and deposition dynamics studies: examples from estuarine and fluvial systems', 'Year': '2005'}\n",
      "\n",
      "{'StaffName': 'Professor Damian Lawler', 'Titles': 'Advances in the continuous monitoring of erosion and deposition dynamics: Developments and applications of the new PEEP-3T system', 'Year': '2008'}\n",
      "\n",
      "{'StaffName': 'marco van de wiel', 'Titles': 'Numerical simulation of bank erosion and channel migration in meandering rivers', 'Year': '2002'}\n",
      "\n",
      "{'StaffName': 'marco van de wiel', 'Titles': 'Numerical modeling of bed topography and bank erosion along tree-lined meandering rivers', 'Year': '2004'}\n",
      "\n",
      "{'StaffName': 'Thet Thet Lin', 'Titles': 'Extreme telomere erosion in ATM-mutated and 11q-deleted CLL patients is independent of disease stage', 'Year': '2012'}\n",
      "\n",
      "{'StaffName': 'Naveed Iqbal', 'Titles': 'The erosion performance of cold spray deposited metal matrix composite coatings with subsequent friction stir processing', 'Year': '2017'}\n",
      "\n",
      "{'StaffName': 'Naveed Iqbal', 'Titles': 'Enhanced erosion performance of cold spray co-deposited AISI316 MMCs modified by friction stir processing', 'Year': '2017'}\n",
      "\n",
      "{'StaffName': 'Naveed Iqbal', 'Titles': 'The erosion performance of particle reinforced metal matrix composite coatings produced by co-deposition cold gas dynamic spraying', 'Year': '2017'}\n",
      "\n",
      "{'StaffName': 'Golnaz Taghavi Pourian Azar', 'Titles': 'The role of droplets on the cavitation erosion damage of TiN coatings produced with cathodic arc physical vapor deposition', 'Year': '2017'}\n",
      "\n",
      "{'StaffName': 'Sam McMaster', 'Titles': 'Probing fatigue resistance in multi-layer DLC coatings by micro-and nano-impact: Correlation to erosion tests', 'Year': '2020'}\n",
      "\n",
      "About 17 results in 0.19 seconds\n"
     ]
    }
   ],
   "source": [
    "# enter the search words\n",
    "query = input(\"ENTER QUERY: \", )\n",
    "print()\n",
    "\n",
    "import time\n",
    "\n",
    "# Beginning time of execution\n",
    "start = time.time()\n",
    "\n",
    "# Instantiate the search engine class\n",
    "search_engine = SearchEngine(publication_titles, stopwords, query)\n",
    "\n",
    "# get the results of the search\n",
    "publication_results = search_engine.publication_search()\n",
    "\n",
    "# display returned results\n",
    "for r in publication_results:\n",
    "    print(dict(processed_publications.loc[r]))\n",
    "    print()\n",
    "    \n",
    "end = time.time()\n",
    "print(\"About %2d results in %.2f seconds\" % (len(publication_results), (end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "google_scholar_crawler.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
